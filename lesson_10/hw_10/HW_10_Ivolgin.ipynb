{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8CpnPep_4yW"
      },
      "source": [
        "#HW 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Задание по итогам курса:**\n",
        "\n",
        "- (упрощенное/для тех, у кого нет вебкамеры)\n",
        "    1. Нужно написать приложение, которое будет получать на вход *изображение*.\n",
        "    2. В процессе определять, что перед камерой находится человек, задетектировав его лицо на кадре.\n",
        "    3. На изображении человек показывает жесты руками, а алгоритм должен считать их и классифицировать.\n",
        "\n",
        "- (более сложное)\n",
        "    1. Нужно написать приложение, которое будет считывать и выводить кадры с *веб-камеры*.\n",
        "    2. В процессе считывания определять что перед камерой находится человек, задетектировав его лицо на кадре.\n",
        "    3. Человек показывает жесты руками, а алгоритм должен считать их и классифицировать. \n",
        "____________________\n",
        "- Для распознавания жестов, вам надо будет скачать датасет https://www.kaggle.com/gti-upm/leapgestrecog, разработать модель для обучения и обучить эту модель.\n",
        "- Как работать с веб-камерой на google colab https://stackoverflow.com/questions/54389727/opening-web-camera-in-google-colab\n",
        "\n",
        "\n",
        "У кого нет возможности работать через каггл (нет верификации), то можете данные взять по ссылке: https://disk.yandex.ru/d/R2PGlaXDf6_HzQ"
      ],
      "metadata": {
        "id": "sAFjwt1EirGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "TCJLClSk63iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch==2.5.2"
      ],
      "metadata": {
        "id": "eQN5Gk_D64XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from glob import glob\n",
        "import sys, os\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from facenet_pytorch import MTCNN"
      ],
      "metadata": {
        "id": "pv8454kp8tY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "images = glob('/content/drive/MyDrive/leapGestRecog/leapGestRecog/**/**/*.png')"
      ],
      "metadata": {
        "id": "xN8C8p8XH2mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [int(os.path.basename(img).split('_')[2])-1 for img in images] # отнимаем 1, чтобы были классы с 0\n",
        "\n",
        "images[:5], labels[:5]"
      ],
      "metadata": {
        "id": "CBKRwgNYHHcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "id": "7QWtNTGobl65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_nums_names = [(int((os.path.split(img)[0].split('/')[-1].split('_'))[0])-1, os.path.split(img)[0].split('/')[-1].split('_')[1:]) for img in images]\n",
        "nums_names_classes = {}\n",
        "for num, name in class_nums_names:\n",
        "    if num not in nums_names_classes.keys():\n",
        "        nums_names_classes[num] = name\n",
        "        \n",
        "sorted(nums_names_classes.items(), key = lambda x: x[0])"
      ],
      "metadata": {
        "id": "9w0Ve4sAbjTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_to_classname(num):\n",
        "    if num == 0:\n",
        "        return 'palm'\n",
        "    elif num == 1:\n",
        "        return 'l'\n",
        "    elif num == 2:\n",
        "        return 'fist'\n",
        "    elif num == 3:\n",
        "        return 'fist_moved'\n",
        "    elif num == 4:\n",
        "        return 'thumb'\n",
        "    elif num == 5:\n",
        "        return 'index'\n",
        "    elif num == 6:\n",
        "        return 'ok'\n",
        "    elif num == 7:\n",
        "        return 'palm_moved'\n",
        "    elif num == 8:\n",
        "        return 'c'\n",
        "    elif num == 9:\n",
        "        return 'down'"
      ],
      "metadata": {
        "id": "Mx5aIC9QAviZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train test split\n",
        "\n",
        "test_size = 0.3\n",
        "random_state = 1\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=test_size, random_state=random_state)"
      ],
      "metadata": {
        "id": "sv42Ox-Ebp2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(images[3])\n",
        "print(np.array(img).shape)\n",
        "img"
      ],
      "metadata": {
        "id": "uDr-uwCBA5pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "iJJEeQ3eMCQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform train & test data\n",
        "\n",
        "train_transformer = transforms.Compose([\n",
        "                         transforms.Grayscale(num_output_channels=1),\n",
        "                         transforms.Resize((48,48)),\n",
        "                         transforms.RandomHorizontalFlip(),           \n",
        "                         transforms.RandomRotation(30),               \n",
        "                         transforms.ToTensor()]) \n",
        "\n",
        "val_transformer =  transforms.Compose([  \n",
        "                         transforms.Grayscale(num_output_channels=1),\n",
        "                         transforms.Resize((48,48)),\n",
        "                         transforms.ToTensor()]) "
      ],
      "metadata": {
        "id": "iOvia5TEMBC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GestureDataset(Dataset):\n",
        "    def __init__(self, images_gestures, labels, transformer):      \n",
        "        self.images = images_gestures\n",
        "        self.labels = labels\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img =  Image.open(self.images[idx]) \n",
        "        img = self.transformer(img)\n",
        "      \n",
        "        return img, self.labels[idx]"
      ],
      "metadata": {
        "id": "Hb_cnEhiM4QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = GestureDataset(X_train, y_train, train_transformer)\n",
        "val_dataset = GestureDataset(X_val, y_val, val_transformer)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle = True, drop_last = True)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "IMFv8NloNHyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(train_loader):\n",
        "    print(f'Класс: {data[1][i]} - {num_to_classname(data[1][i])}')\n",
        "    print(data[0][i].shape)\n",
        "    plt.imshow(data[0][i].permute(1,2,0).squeeze())\n",
        "    plt.show()\n",
        "    if i>5:\n",
        "        break"
      ],
      "metadata": {
        "id": "_-hsOKpJNW32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_batch(train_loader):\n",
        "  \n",
        "    for images, labels in train_loader:\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        print(f'image: {images[0].shape}')\n",
        "        ax.imshow(make_grid(images[:16], nrow=4).permute(1, 2, 0))\n",
        "        break\n",
        "\n",
        "show_batch(train_loader) "
      ],
      "metadata": {
        "id": "oO2y4mBlNsAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "uq3o9ulwN_ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "241rAwhjcoRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class ResNet\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    \n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ELU(inplace=True)]\n",
        "    if pool:\n",
        "        layers.append(nn.MaxPool2d(2))\n",
        "        \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "def linear_block(input_dim, output_dim, activation = False, dropout = 0.3):\n",
        "    \n",
        "    layers = [nn.Linear(input_dim, output_dim),\n",
        "             nn.Dropout(dropout)]\n",
        "    if activation:\n",
        "        layers.append(nn.ReLU())\n",
        "        \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 128)\n",
        "        self.conv2 = conv_block(128, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256)\n",
        "        self.conv4 = conv_block(256, 256, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv5 = conv_block(256, 512)\n",
        "        self.conv6 = conv_block(512, 512, pool=True)\n",
        "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        self.drop3 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(6), \n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.drop1(out)\n",
        "        \n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.drop2(out)\n",
        "        \n",
        "        out = self.conv5(out)\n",
        "        out = self.conv6(out)\n",
        "        out = self.res3(out) + out\n",
        "        out = self.drop3(out)\n",
        "        \n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "BqUOkvC3N96y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize & compile the model\n",
        "\n",
        "net = ResNet(1, len(nums_names_classes)).to(device)\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(net.parameters(), learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "9foycAlwOVRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (img, label) in enumerate(train_loader):\n",
        "    output = net(img[i][None].to(device))\n",
        "    print(f'Real gesture: {int(label[i]), num_to_classname(int(label[i]))}, '\n",
        "          f'Predicted gesture: {int(output.argmax(1)), num_to_classname(int(output.argmax(1)))}')\n",
        "    plt.imshow(img[i].permute(1,2,0).squeeze())\n",
        "    break"
      ],
      "metadata": {
        "id": "2fXfA1W8cirI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model learning\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "epoch_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    train_running_loss = 0.0\n",
        "    total_acc_train = 0.0\n",
        "    epoch_loss = []\n",
        "    for data, labels in tqdm(train_loader):\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        train_running_loss += loss.item()\n",
        "        epoch_loss.append(loss.item())\n",
        "        \n",
        "        acc = (outputs.argmax(dim=1) == labels).sum().item()\n",
        "        total_acc_train += acc\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "    val_running_loss, total_acc_val = 0.0, 0.0\n",
        "    val_epoch_loss = []\n",
        "    for data, labels in val_loader:\n",
        "        net.eval()\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = net(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "                \n",
        "        val_running_loss += loss.item()\n",
        "        val_epoch_loss.append(loss.item())\n",
        "        \n",
        "        acc = (outputs.argmax(dim=1) == labels).sum().item()\n",
        "        total_acc_val += acc\n",
        "        \n",
        "    print(f'Epoch {epoch+1}, loss:, {np.mean(epoch_loss)}, Train acc:, {total_acc_train / len(train_dataset):.3f}  '\n",
        "          f'Val loss:, {np.mean(val_epoch_loss)}, Val acc: {total_acc_val / len(val_dataset):.3f}')\n",
        "    epoch_losses.append(epoch_loss)"
      ],
      "metadata": {
        "id": "GsrNAwj_Okdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss\n",
        "\n",
        "losses = [np.mean(loss) for loss in epoch_losses]\n",
        "plt.plot(losses, '-x')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('losses');"
      ],
      "metadata": {
        "id": "lxwiNHLhOtNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "\n",
        "model_path = './gesture_classification_model.pth'\n",
        "torch.save(net, model_path)"
      ],
      "metadata": {
        "id": "qTEYgsVTPyyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "\n",
        "net = torch.load(model_path)\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(val_loader, 3):\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        plt.title(f'pred - {num_to_classname(outputs[0].argmax())}, gt - {num_to_classname(labels[0])}')\n",
        "        plt.imshow(images[0].cpu().squeeze(), cmap='gray')\n",
        "        plt.show()\n",
        "        if i>10:\n",
        "            break"
      ],
      "metadata": {
        "id": "qF1QEqmKP7Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "uVM3NV7uQ1QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "from facenet_pytorch import MTCNN"
      ],
      "metadata": {
        "id": "kRLNQmM7R4z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    \n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ELU(inplace=True)]\n",
        "    if pool:\n",
        "        layers.append(nn.MaxPool2d(2))\n",
        "        \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "# Блок линейных слоев\n",
        "def linear_block(input_dim, output_dim, activation = False, dropout = 0.3):\n",
        "    \n",
        "    layers = [nn.Linear(input_dim, output_dim),\n",
        "             nn.Dropout(dropout)]\n",
        "    if activation:\n",
        "        layers.append(nn.ReLU())\n",
        "        \n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "#  Сеть\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Конволюционная часть сети\n",
        "        self.conv1 = conv_block(in_channels, 128)\n",
        "        self.conv2 = conv_block(128, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        self.drop1 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256)\n",
        "        self.conv4 = conv_block(256, 256, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(256, 256), conv_block(256, 256))\n",
        "        self.drop2 = nn.Dropout(0.5)\n",
        "        \n",
        "        self.conv5 = conv_block(256, 512)\n",
        "        self.conv6 = conv_block(512, 512, pool=True)\n",
        "        self.res3 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        self.drop3 = nn.Dropout(0.5)\n",
        "\n",
        "        \n",
        "        self.max_pool = nn.MaxPool2d(6)\n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        self.ff1 = linear_block(512, 256, activation = True)\n",
        "        self.ff2 = linear_block(256, 128, activation = True)\n",
        "        self.classifier = linear_block(128, num_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        #Convolution layers\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) \n",
        "        out = self.drop1(out)\n",
        "        \n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.drop2(out)\n",
        "        \n",
        "        out = self.conv5(out)\n",
        "        out = self.conv6(out)\n",
        "        out = self.res3(out) + out\n",
        "        out = self.drop3(out)\n",
        "\n",
        "        out = self.max_pool(out)\n",
        "        out = self.flatten(out)\n",
        "        \n",
        "        # Feed Forward layers\n",
        "        out = self.ff1(out)\n",
        "        out = self.ff2(out)\n",
        "        out = self.classifier(out)\n",
        "        \n",
        "        return out"
      ],
      "metadata": {
        "id": "-Ljdscm_Q1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gesture_clf = ResNet(1, 10)\n",
        "gesture_clf = torch.load(model_path)"
      ],
      "metadata": {
        "id": "g5xxB1nnSAaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class FaceDetector\n",
        "\n",
        "class FaceDetector(object):\n",
        "    \"\"\"\n",
        "    Face detector class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mtcnn):\n",
        "        self.mtcnn = mtcnn\n",
        "        self.gestmodel = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "        self.gestmodel.eval()\n",
        "\n",
        "    def _draw(self, frame, boxes, landmarks, gesture):\n",
        "        \"\"\"\n",
        "        Draw landmarks and boxes for each face detected\n",
        "        \"\"\"\n",
        "        try:\n",
        "            for box, ld in zip(boxes, landmarks):\n",
        "                cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), \n",
        "                              (255,255,0), thickness=2)\n",
        "\n",
        "                cv2.putText(frame, \n",
        "                    gesture, (int(box[2]), int(box[3])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[0]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[1]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[2]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[3]]), 5, (255,255,0), -1)\n",
        "                cv2.circle(frame, tuple([int(i) for i in ld[4]]), 5, (255,255,0), -1)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return frame\n",
        "    \n",
        "    @staticmethod\n",
        "    def crop_faces(frame, boxes):\n",
        "        faces = []\n",
        "        for i, box in enumerate(boxes):\n",
        "            faces.append(frame[int(box[1]):int(box[3]), \n",
        "                int(box[0]):int(box[2])])\n",
        "        return faces\n",
        "    \n",
        "    @staticmethod\n",
        "    def transform_frame(frame):\n",
        "        transformer = transforms.Compose([  \n",
        "                      transforms.ToPILImage(),\n",
        "                      transforms.Grayscale(num_output_channels=1),\n",
        "                      transforms.Resize((48,48)),\n",
        "                      transforms.ToTensor()]) \n",
        "        \n",
        "        return transformer(frame).unsqueeze(1)\n",
        "        \n",
        "    @staticmethod\n",
        "    def num_to_classname(num):\n",
        "        if num == 0:\n",
        "            return 'palm'\n",
        "        elif num == 1:\n",
        "            return 'l'\n",
        "        elif num == 2:\n",
        "            return 'fist'\n",
        "        elif num == 3:\n",
        "            return 'fist_moved'\n",
        "        elif num == 4:\n",
        "            return 'thumb'\n",
        "        elif num == 5:\n",
        "            return 'index'\n",
        "        elif num == 6:\n",
        "            return 'ok'\n",
        "        elif num == 7:\n",
        "            return 'palm_moved'\n",
        "        elif num == 8:\n",
        "            return 'c'\n",
        "        elif num == 9:\n",
        "            return 'down'\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def reactions_to_gestures(gesture):\n",
        "        if gesture == 'thumb':\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/good jıb.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "            print(f\"Hi! See your photo below, you're amazing today :D. Your gesture is {gesture}, am I right?\")\n",
        "            \n",
        "        elif gesture == 'ok':\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/ok.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "            print(f\"Hey, you're showing OK! See your photo below! :D. I think, your gesture is {gesture}, am I right?\")\n",
        "        \n",
        "        elif gesture == 'palm' or gesture == 'palm_moved':\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/palm.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "            print(f\"Hi! See your photo below, you're amazing today :D. Your gesture is {gesture}, am I right?\")\n",
        "            \n",
        "        elif gesture == 'fist' or  gesture == 'fist_moved':\n",
        "            print(f\"Hey, why you angry today? :( I think, your gesture is {gesture}, am I right?\")\n",
        "            plt.imshow(cv2.cvtColor(cv2.imread('./imgs reactions/fist.jpeg'), cv2.COLOR_BGR2RGB))\n",
        "            plt.show()\n",
        "    \n",
        "        else:\n",
        "            game(15)\n",
        "            print(f'Hi! I think, your gesture is {gesture}, am I right?')\n",
        "              \n",
        "    \n",
        "    @staticmethod\n",
        "    def game(turns):\n",
        "        print(f\"Let's play the game! Try to guess the word letter by letter!You have {turns} attempts!\")\n",
        "    \n",
        "        want_to_play= input('Do you want to play? Enter Y or N: ')[0]\n",
        "        want_to_play.lower()\n",
        "\n",
        "        if want_to_play == 'y':\n",
        "\n",
        "            wordList = [\"python\", \"pytorch\", \"nets\", \"framework\"]\n",
        "            shuffle(wordList)\n",
        "            word = wordList.pop()\n",
        "\n",
        "            guesses = \"\" \n",
        "\n",
        "            while turns > 0:\n",
        "                wrong = 0\n",
        "\n",
        "                for letter in word:\n",
        "                    if letter in guesses:\n",
        "                        print(letter, end=\" \")\n",
        "                    else:\n",
        "                        print(\"_\", end=\" \")\n",
        "                        wrong += 1\n",
        "\n",
        "                print(\"\\n\")\n",
        "\n",
        "                if wrong == 0:\n",
        "                    print(\"You win!!! :)\")\n",
        "                    break\n",
        "\n",
        "                guess = \"\"\n",
        "                guess = input(\"Enter the ENGLISH letter and press 'enter': \")[0]\n",
        "                guess.lower()\n",
        "                if guess in guesses:\n",
        "                    print(\"You had entered this letter before\")\n",
        "                guesses += guess\n",
        "\n",
        "                turns -=1\n",
        "                if guess not in word:\n",
        "                    print(f\"No letter {guess} in this word \")\n",
        "\n",
        "                else:\n",
        "                    print(f\"Correct! Letter {guess} contains in our word \")\n",
        "\n",
        "\n",
        "                print(f\"You have {turns} attempts left\")\n",
        "\n",
        "\n",
        "                if turns == 0:\n",
        "                    print(\"Sorry, you lose :(\")\n",
        "\n",
        "        if want_to_play == 'n': \n",
        "            print('See you next time! Bye;)')\n",
        "                \n",
        "                  \n",
        "    def run(self):\n",
        "             \n",
        "        try:\n",
        "            cap = cv2.VideoCapture(0)\n",
        "            for i in range(3):\n",
        "                cap.read()\n",
        "            ret, frame = cap.read()\n",
        "\n",
        "            boxes, probs, landmarks = self.mtcnn.detect(frame, landmarks=True)\n",
        "\n",
        "            face = self.crop_faces(frame, boxes)[0]\n",
        "\n",
        "            frame_for_model = self.transform_frame(frame)\n",
        "            gesture = self.gestmodel(frame_for_model)\n",
        "            gesture = self.num_to_classname(gesture.argmax())\n",
        "            self.reactions_to_gestures(gesture)\n",
        "\n",
        "            self._draw(frame, boxes, landmarks, gesture)\n",
        "\n",
        "            plt.imshow(frame)\n",
        "            plt.show()\n",
        "\n",
        "            \n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "TWG_vEFOSFhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST run\n",
        "mtcnn = MTCNN()\n",
        "fcd = FaceDetector(mtcnn)\n",
        "fcd.run()"
      ],
      "metadata": {
        "id": "pHAJwOEMSL4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Не получилось запустить видео"
      ],
      "metadata": {
        "id": "UkM6AHzeSOqH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "HW_10_Ivolgin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}